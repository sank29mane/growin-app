---
phase: 18-adversarial-alpha
plan: 02
type: execute
wave: 2
depends_on: [18-01]
files_modified: [backend/agents/orchestrator_agent.py, backend/agents/risk_agent.py]
autonomous: true
requirements: [ADV-02]

must_haves:
  truths:
    - "RiskAgent pushes back on Orchestrator's thesis at least once"
    - "ACE score is calculated and appended to Orchestrator's output"
  artifacts:
    - path: "backend/agents/risk_agent.py"
      provides: "The Contrarian persona and iterative critique logic"
    - path: "backend/agents/orchestrator_agent.py"
      provides: "Multi-turn debate management and ACE calculation"
  key_links:
    - from: "OrchestratorAgent"
      to: "RiskAgent"
      via: "Iterative debate loop (Max 2 turns)"
    - from: "Adversarial Robustness"
      to: "ACE Score"
      via: "Weighted calculation of critique resolution"
---

<objective>
Implement the Adversarial Reasoning Engine using the 'Critic Pattern' to create a multi-turn debate between RiskAgent and OrchestratorAgent.

Purpose: Increase recommendation robustness through adversarial pressure.
Output: Integrated Multi-turn debate logic and ACE (Adversarial Confidence Estimation) score.
</objective>

<execution_context>
@/Users/sanketmane/.gemini/get-shit-done/workflows/execute-plan.md
@/Users/sanketmane/.gemini/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@backend/agents/orchestrator_agent.py
@backend/agents/risk_agent.py
@.gsd/RESEARCH_PHASE_18.md
@.planning/phases/18-adversarial-alpha/18-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: RiskAgent 'The Contrarian' Persona</name>
  <files>backend/agents/risk_agent.py</files>
  <action>
    - Update RiskAgent's system prompt to explicitly define "The Contrarian" persona.
    - This persona must look for reasons why the Orchestrator's thesis is WRONG, focusing on hidden risks, logic gaps, or tail-risk events.
    - Enhance the output JSON to include a 'debate_refutation' field containing the adversarial argument.
  </action>
  <verify>
    Verify that RiskAgent.analyze returns a 'debate_refutation' when provided with a bullish suggestion.
  </verify>
  <done>
    RiskAgent can generate targeted adversarial refutations.
  </done>
</task>

<task type="auto">
  <name>Task 2: Orchestrator Multi-turn Debate & ACE Score</name>
  <files>backend/agents/orchestrator_agent.py</files>
  <action>
    - Modify OrchestratorAgent.run() and run_stream() to implement the debate loop (max 2 turns).
    - If RiskAgent flags/critiques the thesis, Orchestrator must generate a "defense" or "refutation response" and send it back to RiskAgent for a final check.
    - Calculate the **ACE (Adversarial Confidence Estimation)** score based on:
        - 0.0 to 1.0 (Robustness of the thesis after critiques).
        - Higher score if RiskAgent eventually approves or if Orchestrator provides evidence that resolves the critique.
    - Append ACE Score and a "Debate Trace" summary to the final output.
  </action>
  <verify>
    Trace a conversation and check if the final output contains "ACE Score" and "Reasoning Trace" segments.
  </verify>
  <done>
    Orchestrator successfully manages a multi-turn debate with RiskAgent and provides a robustness score.
  </done>
</task>

</tasks>

<verification>
Check OrchestratorAgent logs to see the iterative debate (messages going between agents).
</verification>

<success_criteria>
Trades and recommendations undergo adversarial pressure; users see the ACE score as a metric of confidence.
</success_criteria>

<output>
After completion, create `.planning/phases/18-adversarial-alpha/18-02-SUMMARY.md`
</output>
