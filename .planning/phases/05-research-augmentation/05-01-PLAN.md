---
phase: 05-research-augmentation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified: [backend/lm_studio_client.py]
autonomous: true
requirements: [LM-01, LM-02, LM-03]
must_haves:
  truths:
    - "LMStudioClient supports parallel concurrent requests via native asyncio and batching"
    - "LMStudioClient enforces 60% RAM rule safety for M4 Pro memory bandwidth"
    - "LMStudioClient implements content-based prefix caching for TTFT reduction"
  artifacts:
    - path: "backend/lm_studio_client.py"
      provides: "LM Studio 0.4.0+ optimized REST client"
  key_links:
    - from: "backend/lm_studio_client.py"
      to: "LM Studio REST API"
      via: "httpx"
---

<objective>
Refactor `lm_studio_client.py` to support LM Studio 0.4.0+ optimized concurrency features, specifically parallel requests, continuous batching, and content-based prefix caching. Integrate memory safety logic for M4 Pro (60% RAM rule).

Purpose: Maximize inference throughput and minimize Time-To-First-Token (TTFT) for multi-agent consultations.
Output: High-performance, memory-aware LM Studio client.
</objective>

<execution_context>
@/Users/sanketmane/.gemini/get-shit-done/workflows/execute-plan.md
</execution_context>

<context>
@backend/lm_studio_client.py
@backend/model_config.py
@.planning/STATE.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement Parallel Concurrency & 60% RAM Rule</name>
  <files>backend/lm_studio_client.py</files>
  <action>
    - Refactor `LMStudioClient` to dynamically calculate `max_concurrent_predictions`.
    - Implement the "60% RAM rule": `Max Concurrent = (Total System RAM * 0.6 - Model Size) / KV Cache per Request`.
    - Use `psutil` or similar to detect total RAM. Assume standard KV cache per request (e.g. 512MB-1GB depending on context).
    - Add a Semaphore to `LMStudioClient` to limit concurrent `chat` calls based on this rule.
  </action>
  <verify>Check `lm_studio_client.py` for Semaphore implementation and RAM-based calculation.</verify>
  <done>Client limits concurrency based on available system memory to prevent swapping.</done>
</task>

<task type="auto">
  <name>Task 2: Implement Content-Based Prefix Caching</name>
  <files>backend/lm_studio_client.py</files>
  <action>
    - Update `chat` method to include headers or payload flags for Content-Based Prefix Caching (LM Studio 0.4.0+ feature).
    - Add `"cache_prompt": true` to the payload if the system prompt is consistent.
    - Ensure that `system_prompt` is handled separately from `messages` if needed to optimize cache hits.
  </action>
  <verify>Verify payload includes prefix caching signals.</verify>
  <done>TTFT reduced for repeated system prompts via prefix caching.</done>
</task>

<task type="auto">
  <name>Task 3: Refactor Batch Chat for Continuous Batching</name>
  <files>backend/lm_studio_client.py</files>
  <action>
    - Refactor `batch_chat` to leverage the internal Semaphore and `asyncio.gather`.
    - Ensure it provides a robust way to trigger multiple independent model completions in parallel.
    - Add better error handling for "Channel Error" or crashes during high-concurrency bursts.
  </action>
  <verify>Run a test script with 5+ concurrent requests to `batch_chat`.</verify>
  <done>Batch chat executes multiple requests in parallel without exceeding memory limits.</done>
</task>

</tasks>

<verification>
Check for:
1. Memory limit logic in `__init__` or a helper.
2. Semaphore usage in `chat`.
3. Prefix caching flags in API payload.
</verification>

<success_criteria>
- Concurrent requests handled safely within RAM budget.
- Prefix caching implemented for performance.
</success_criteria>

<output>
After completion, create `.planning/phases/05-research-augmentation/05-01-SUMMARY.md`
</output>
